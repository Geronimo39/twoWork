import time
import csv
import requests
from bs4 import BeautifulSoup

# ---------------------------------------------------------
# KONFIGURATION
# ---------------------------------------------------------

MAIN_URL = "https://intranet.firma.de/geburtstage"   # <-- anpassen

# Testmodus: True = nur 1 Unterseite, False = alle Unterseiten
TESTMODE = True

# Eigener User-Agent
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0 Safari/537.36"
    )
}

# Session fÃ¼r Cookies / Login
session = requests.Session()

# Rate-Limit
RATE_LIMIT_SECONDS = 1.5


# ---------------------------------------------------------
# FUNKTIONEN
# ---------------------------------------------------------

def fetch_page(url):
    """LÃ¤dt eine Seite mit Rate-Limit und User-Agent."""
    print(f"Lade Seite: {url}")
    time.sleep(RATE_LIMIT_SECONDS)

    response = session.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.text


def extract_subpage_links(html):
    """Extrahiert alle alphabetischen Unterseiten-Links."""
    soup = BeautifulSoup(html, "html.parser")

    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # Filter: nur Links, die zu Geburtstags-Unterseiten gehÃ¶ren
        if "geburtstage" in href and href != MAIN_URL:
            links.append(href)

    return links


def parse_birthdays(html):
    """Extrahiert Namen + Geburtstage aus einer Unterseite."""
    soup = BeautifulSoup(html, "html.parser")

    rows = soup.find_all("tr")
    birthdays = []

    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 2:
            name = cols[0].get_text(strip=True)
            birthday = cols[1].get_text(strip=True)
            birthdays.append((name, birthday))

    return birthdays


def export_to_csv(data, filename="birthdays.csv"):
    """Speichert Geburtstage in einer CSV-Datei."""
    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Name", "Geburtstag"])  # Kopfzeile
        writer.writerows(data)

    print(f"\nCSV-Datei gespeichert als: {filename}")


# ---------------------------------------------------------
# HAUPTPROGRAMM
# ---------------------------------------------------------

# 1. Hauptseite laden
main_html = fetch_page(MAIN_URL)

# 2. Unterseiten-Links extrahieren
subpages = extract_subpage_links(main_html)

print("Gefundene Unterseiten:")
for link in subpages:
    print(" -", link)

# 3. Testmodus: nur die erste Unterseite
if TESTMODE:
    print("\nTESTMODUS AKTIV: Es wird nur die erste Unterseite verarbeitet.\n")
    subpages = subpages[:1]
else:
    print("\nPRODUKTIVMODUS: Alle Unterseiten werden verarbeitet.\n")

# 4. Alle (oder nur eine) Unterseiten laden
all_birthdays = []

for url in subpages:
    html = fetch_page(url)
    birthdays = parse_birthdays(html)
    all_birthdays.extend(birthdays)

# 5. CSV exportieren
export_to_csv(all_birthdays)




#----------------------   MARKDOWN   ------------------------------------------
# ðŸ§ª Webâ€‘Scraping & Requests sicher testen (ohne echte Webseiten zu crawlen)

Bevor man interne Firmenwebseiten automatisiert ausliest, ist es sinnvoll,
die grundlegenden Techniken mit **Ã¶ffentlichen Testseiten** zu Ã¼ben.
Diese Seiten sind ausdrÃ¼cklich dafÃ¼r gedacht und vÃ¶llig unkritisch.

---

## âœ… 1. httpbin.org â€“ Responseâ€‘Handling testen

`httpbin.org` ist perfekt, um alles rund um HTTPâ€‘Requests zu Ã¼ben:

- Statuscodes
- Header
- Cookies
- Userâ€‘Agent
- Delay / Rateâ€‘Limit
- JSONâ€‘Antworten

### Beispiel: GETâ€‘Request

```python
import requests

r = requests.get("https://httpbin.org/get")
r.json()
#---------------------   ENDE: MARKDOWN   ------------------------------------------



headers = {"User-Agent": "TestBot/1.0"}
r = requests.get("https://httpbin.org/user-agent", headers=headers)
r.json()
#-----------------------------------------------------------------------
r = requests.get("https://httpbin.org/delay/3")
r.json()

#----------------------------------------------------------------
from bs4 import BeautifulSoup
import requests

html = requests.get("https://example.com").text
soup = BeautifulSoup(html, "html.parser")

soup.find("h1").get_text()

#----------------------------------------------------------------
import requests
from bs4 import BeautifulSoup

html = requests.get("https://books.toscrape.com/").text
soup = BeautifulSoup(html, "html.parser")

titles = [h3.get_text(strip=True) for h3 in soup.find_all("h3")]
titles[:10]
#----------------------------------------------------------------

import requests
from bs4 import BeautifulSoup

html = requests.get("https://quotes.toscrape.com/").text
soup = BeautifulSoup(html, "html.parser")

quotes = [q.get_text(strip=True) for q in soup.find_all("span", class_="text")]
quotes[:5]
#----------------------------------------------------------------

url = "https://raw.githubusercontent.com/<user>/<repo>/main/README.md"
text = requests.get(url).text
print(text[:500])
#----------------------------------------------------------------
requests.get("https://<user>.github.io/<repo>/").text
#----------------------------------------------------------------

import time
import requests

session = requests.Session()

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0 Safari/537.36"
    )
}

RATE_LIMIT_SECONDS = 1.5

def fetch(url):
    print("Lade:", url)
    time.sleep(RATE_LIMIT_SECONDS)
    r = session.get(url, headers=HEADERS)
    r.raise_for_status()
    return r

# Testaufruf
response = fetch("https://httpbin.org/get")
response.json()
