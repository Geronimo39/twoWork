ibmi_search/
  ibmi_search.py
  worker.py
  requirements.txt
  README.md

#-------------------------------------------------------------

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, Optional, Literal
import re
import time
import pandas as pd
import pyodbc

from worker import scan_one_table_sample


SearchMode = Literal["search_pattern", "sample_scan"]


@dataclass(frozen=True)
class OdbcConfig:
    dsn: str
    uid: str
    pwd: str

    def connect(self) -> pyodbc.Connection:
        # Tipp: Credentials lieber aus ENV-Variablen lesen statt hardcoden.
        return pyodbc.connect(f"DSN={self.dsn};UID={self.uid};PWD={self.pwd}")


def list_tables(conn: pyodbc.Connection, schema: str) -> pd.DataFrame:
    sql = """
    SELECT TABLE_SCHEMA, TABLE_NAME
    FROM QSYS2.SYSTABLES
    WHERE TABLE_SCHEMA = ?
      AND TABLE_TYPE IN ('T', 'P')  -- T=Table, P=Physical file (typisch auf IBM i)
    ORDER BY TABLE_NAME
    """
    return pd.read_sql(sql, conn, params=[schema])


def search_with_search_pattern(
    conn: pyodbc.Connection,
    schema: str,
    pattern: str,
) -> pd.DataFrame:
    sql = f"""
    SELECT *
    FROM TABLE(QSYS2.SEARCH_PATTERN(
        PATTERN => ?,
        SCHEMA  => ?
    )) AS X
    """
    # Ergebnisstruktur h√§ngt von IBM i Version/Paketstand ab; DataFrame ist robust genug.
    return pd.read_sql(sql, conn, params=[pattern, schema])


def search_sample_scan(
    odbc: OdbcConfig,
    schema: str,
    pattern: str,
    *,
    max_rows_per_table: int = 200,
    max_tables: Optional[int] = None,
    case_insensitive: bool = True,
    use_regex: bool = False,
    processes: int = 6,
) -> pd.DataFrame:
    """
    L√§dt je Tabelle nur die ersten N Zeilen und sucht spaltenweise (kein Zeilen-String-Join).
    Multiprocessing l√§uft stabil, weil Worker-Funktion in worker.py top-level liegt.
    """
    import multiprocessing as mp

    with odbc.connect() as conn:
        tables_df = list_tables(conn, schema)

    if max_tables is not None:
        tables_df = tables_df.head(max_tables)

    tables = list(tables_df.itertuples(index=False, name=None))  # (schema, table)

    flags = re.IGNORECASE if case_insensitive else 0
    compiled = re.compile(pattern, flags=flags) if use_regex else None

    t0 = time.time()

    # Wichtig: Jede Process-Worker-Instanz baut ihre eigene ODBC-Verbindung auf.
    # Das ist bei ODBC i.d.R. stabiler als eine Connection zu sharen.
    with mp.get_context("spawn").Pool(processes=processes) as pool:
        jobs = [
            (odbc, schema_name, table_name, pattern, compiled, use_regex, max_rows_per_table)
            for (schema_name, table_name) in tables
        ]
        results = pool.map(scan_one_table_sample, jobs)

    out = pd.concat([r for r in results if r is not None], ignore_index=True)
    out["runtime_seconds"] = round(time.time() - t0, 3)
    return out


def run_search(
    mode: SearchMode,
    odbc: OdbcConfig,
    schema: str,
    pattern: str,
    *,
    max_rows_per_table: int = 200,
    processes: int = 6,
    use_regex: bool = False,
) -> pd.DataFrame:
    with odbc.connect() as conn:
        if mode == "search_pattern":
            return search_with_search_pattern(conn, schema=schema, pattern=pattern)

    if mode == "sample_scan":
        return search_sample_scan(
            odbc=odbc,
            schema=schema,
            pattern=pattern,
            max_rows_per_table=max_rows_per_table,
            processes=processes,
            use_regex=use_regex,
        )

    raise ValueError(f"Unknown mode: {mode}")


#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
# W O R K E R:
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple, Any
import pandas as pd
import pyodbc
import re


def _connect(odbc_cfg) -> pyodbc.Connection:
    return pyodbc.connect(f"DSN={odbc_cfg.dsn};UID={odbc_cfg.uid};PWD={odbc_cfg.pwd}")


def _quote_ident(name: str) -> str:
    # IBM i / Db2: doppelte Quotes f√ºr Identifiers sind Standard-SQL-sicher.
    # Achtung: Das sch√ºtzt NICHT gegen beliebige SQL-Injection, wenn name untrusted ist.
    return '"' + name.replace('"', '""') + '"'


def _select_first_n(schema: str, table: str, n: int) -> str:
    # IBM i unterst√ºtzt FETCH FIRST n ROWS ONLY.
    # Reihenfolge ist ohne ORDER BY nicht determiniert; f√ºr "quick sniff" ok.
    return f"""
    SELECT *
    FROM {_quote_ident(schema)}.{_quote_ident(table)}
    FETCH FIRST {int(n)} ROWS ONLY
    """


def _vectorized_contains(df: pd.DataFrame, needle: str, *, use_regex: bool, compiled: Optional[re.Pattern]) -> pd.DataFrame:
    # spaltenweise Suche, robust gegen NULLs
    s = df.astype("string")

    if use_regex:
        # compiled kann nicht sauber √ºber Prozesse picklebar sein (je nach Plattform),
        # daher nutzen wir needle als Pattern. (Alternativ: compiled.pattern)
        mask = s.apply(lambda col: col.str.contains(needle, regex=True, na=False))
    else:
        mask = s.apply(lambda col: col.str.contains(needle, regex=False, na=False))

    hit_rows = mask.any(axis=1)
    if not hit_rows.any():
        return df.iloc[0:0]

    return df.loc[hit_rows]


def scan_one_table_sample(args) -> Optional[pd.DataFrame]:
    """
    args:
      (odbc_cfg, schema, table, pattern, compiled, use_regex, max_rows)
    Returns:
      None if no hits; else DataFrame with hits + metadata columns.
    """
    odbc_cfg, schema, table, pattern, compiled, use_regex, max_rows = args

    try:
        with _connect(odbc_cfg) as conn:
            df = pd.read_sql(_select_first_n(schema, table, max_rows), conn)

        hits = _vectorized_contains(df, pattern, use_regex=use_regex, compiled=compiled)
        if hits.empty:
            return None

        # Metadaten dazu, damit du Treffer sauber nachverfolgen kannst
        hits = hits.copy()
        hits.insert(0, "TABLE_SCHEMA", schema)
        hits.insert(1, "TABLE_NAME", table)
        return hits

    except Exception as ex:
        # In produktiven Setups: Logging statt silent failure.
        # Hier geben wir eine kleine Fehlerzeile zur√ºck, damit du siehst, was nicht lesbar war.
        return pd.DataFrame([{
            "TABLE_SCHEMA": schema,
            "TABLE_NAME": table,
            "ERROR": str(ex),
        }])

# ----------------------------------------------------------------------------------------------------

from ibmi_search import OdbcConfig, run_search

odbc = OdbcConfig(dsn="IBMI", uid="DEINUSER", pwd="DEINPASS")

# 1) IBM-i Power-Modus (falls verf√ºgbar/erlaubt)
df = run_search("search_pattern", odbc, schema="DEINEBIB", pattern="SUCHBEGRIFF")
df.head()

# 2) Sample-Scan-Modus (erste 200 Zeilen pro Tabelle, parallel)
df2 = run_search("sample_scan", odbc, schema="DEINEBIB", pattern="SUCHBEGRIFF",
                 max_rows_per_table=200, processes=6, use_regex=False)
df2.head()


# ----------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------------------------
Metadaten: Tabellen und Spalten
-- Tabellen einer Bibliothek
SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TEXT, TABLE_TYPE
FROM QSYS2.SYSTABLES
WHERE TABLE_SCHEMA = 'DEINEBIB'
ORDER BY TABLE_NAME;

-- Spalten einer Tabelle
SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, LENGTH, NUMERIC_SCALE, COLUMN_TEXT
FROM QSYS2.SYSCOLUMNS
WHERE TABLE_SCHEMA = 'DEINEBIB'
  AND TABLE_NAME = 'DEINETAB'
ORDER BY ORDINAL_POSITION;

-- Spalten finden (Name oder Text)
SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, DATA_TYPE, COLUMN_TEXT
FROM QSYS2.SYSCOLUMNS
WHERE TABLE_SCHEMA = 'DEINEBIB'
  AND (UPPER(COLUMN_NAME) LIKE '%KUND%'
       OR UPPER(COALESCE(COLUMN_TEXT, '')) LIKE '%KUND%')
ORDER BY TABLE_NAME, ORDINAL_POSITION;

Datentypen: Welche Spalten sind √ºberhaupt ‚Äûdurchsuchbar‚Äú?
SELECT DATA_TYPE, COUNT(*) AS CNT
FROM QSYS2.SYSCOLUMNS
WHERE TABLE_SCHEMA = 'DEINEBIB'
GROUP BY DATA_TYPE
ORDER BY CNT DESC;

Quick-Sniff pro Tabelle: erste ùëÅ Zeilen
